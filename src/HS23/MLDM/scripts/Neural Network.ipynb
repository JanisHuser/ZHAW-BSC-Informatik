{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights= 12960\n",
      "Biases= 42\n"
     ]
    }
   ],
   "source": [
    "# How many parameters\n",
    "\n",
    "input_layer_nodes = 784\n",
    "output_layer_neurons = 10\n",
    "hidden_layer_count = 2\n",
    "neutrons_per_hidden_layer = 16\n",
    "\n",
    "def calc_weight_count():\n",
    "\tcount = input_layer_nodes * neutrons_per_hidden_layer\n",
    "\n",
    "\tfor _ in range(hidden_layer_count-1):\n",
    "\t\tcount += (neutrons_per_hidden_layer ** 2)\n",
    "\n",
    "\tcount += neutrons_per_hidden_layer * output_layer_neurons\n",
    "\n",
    "\treturn count\n",
    "\n",
    "\n",
    "def calc_bias_count():\n",
    "\tcount = neutrons_per_hidden_layer\n",
    "\tfor _ in range(hidden_layer_count-1):\n",
    "\t\tcount += neutrons_per_hidden_layer\n",
    "\n",
    "\tcount += output_layer_neurons\n",
    "\n",
    "\treturn count\n",
    "\n",
    "print (\"Weights=\", calc_weight_count())\n",
    "print (\"Biases=\", calc_bias_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.1, 0.3, 0.6, 0.7]\n",
    "y = [0.2, 0.25, 0.4, 0.7]\n",
    "w = [0.5]\n",
    "b = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate y_pred with the weight w and bias b\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "\txi = x[i]\n",
    "\n",
    "\ty_pred.append(xi * w[0] + b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04125\n"
     ]
    }
   ],
   "source": [
    "# 2. Calculate MSE\n",
    "\n",
    "def calc_mse(y, y_pred):\n",
    "\tsum = 0\n",
    "\tfor i in range(len(y)):\n",
    "\t\tdifference = y[i] - y_pred[i]\n",
    "\n",
    "\t\tsum += (difference ** 2)\n",
    "\n",
    "\tmse = sum / len(y)\n",
    "\n",
    "\treturn mse\n",
    "\n",
    "mse = calc_mse(y, y_pred)\n",
    "print (mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During backpropagation, we take the partial derivative of the error function with respect to each weight and bias in the model. The error function does not contain any weights or biases in its equation so we use the chain rule to do so. The result of doing this is a direction and magnitude in which each parameter should be tuned to minimize the error function. This concept is called gradient descent.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VcNaP3V8Mr-DOma0JHiS8g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first partial derivative we will compute is the partial derivative of the error function with respect to the activation function h11. This is where we see the the benefit of using 1/2 * MSE. By multiplying by a factor of 1/2 we are eliminating all coefficients (other than 1) in the resulting partial derivative.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wxABy--X7gD92PFcZGRE9g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
