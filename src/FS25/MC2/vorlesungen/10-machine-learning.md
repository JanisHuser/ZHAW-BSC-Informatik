# ğŸ¤– Embedded Machine Learning

## ğŸ¯ Ziel

VerstÃ¤ndnis fÃ¼r die Anwendung von Machine Learning (ML) auf Embedded-Systemen mit eingeschrÃ¤nkten Ressourcen.

## ğŸ§  Grundlagen Neuronaler Netze

### Feed Forward Netze

* Bestehen aus: Eingabeschicht, versteckten Schichten, Ausgabeschicht
* Verbindungen zwischen allen Neuronen jeder Schicht
* KÃ¶nnen beliebige stetige Funktionen approximieren

### Aktivierungsfunktionen

| Funktion   | Formel / Verhalten                  | Verwendung                      |
| ---------- | ----------------------------------- | ------------------------------- |
| Sigmoid    | $\frac{1}{1 + e^{-x}}$              | Klassisch, nicht mehr empfohlen |
| Tanh       | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Besser als Sigmoid              |
| ReLU       | $\max(0, x)$                        | Standard fÃ¼r hidden layers      |
| Leaky ReLU | $\max(\alpha x, x)$                 | Vermeidet "Dead Neurons"        |

## ğŸ“ Beispiel: MNIST Dataset

* Handgeschriebene Ziffern (0â€“9)
* 28Ã—28 Pixel, Graustufen
* Trainingsdaten: 60'000 Bilder, Testdaten: 10'000
* In Keras integriert: `keras.datasets.mnist.load_data()`

## âš™ï¸ Trainingsablauf

1. **Initialisierung**: Gewichtung & Architektur definieren
2. **Forward Pass**: Input â†’ Output berechnen
3. **Loss berechnen**: z.â€¯B. mit `categorical_crossentropy`
4. **Backpropagation**: Gradienten berechnen
5. **Optimierung**: z.â€¯B. mit `Adam`, `SGD`, `RMSprop`
6. **Wiederholung** Ã¼ber viele Epochen

### Codebeispiel mit Keras

```python
model = keras.Sequential([
    keras.Input(shape=(28, 28, 1)),
    layers.Flatten(),
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(10, activation="softmax"),
])
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.fit(x_train, y_train, batch_size=50, epochs=5, verbose=1)
```

## âš™ï¸ Training vs. Inferenz

| Phase    | Beschreibung                                             |
| -------- | -------------------------------------------------------- |
| Training | Nur auf leistungsstarken Systemen                        |
| Inferenz | Optimiert fÃ¼r Embedded (z.â€¯B. Raspberry Pi, Jetson Nano) |

## ğŸ§° Frameworks

| Framework  | Beschreibung                     |
| ---------- | -------------------------------- |
| TensorFlow | MarktfÃ¼hrer, weit verbreitet     |
| Keras      | High-Level API auf TensorFlow    |
| PyTorch    | Flexibel & beliebt bei Forschung |
| TFLite     | Optimiert fÃ¼r Embedded Inferenz  |

## ğŸ–¼ï¸ Praxisszenario

* Kamera â†’ Bild â†’ OpenCV-Preprocessing â†’ TensorFlow-Modell â†’ Klassifikation

## ğŸ§ª Lab Setup

* EingabegerÃ¤t: USB-Kamera
* Hardware: Raspberry Pi
* Training auf PC, Deployment auf Pi

## ğŸª„ Optimierungen fÃ¼r Embedded

* Quantisierung (8-bit statt 32-bit)
* Pruning (Entfernung unwichtiger Gewichte)
* Model Compression (z.â€¯B. mit TFLite Converter)

---

Embedded Machine Learning macht es mÃ¶glich, KI in kleine GerÃ¤te zu bringen â€“ lokal, datensparsam und reaktiv.
